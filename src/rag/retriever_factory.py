# src/rag/retriever_factory.py
from pathlib import Path
from typing import List

from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.tools import tool, BaseTool  # ä¿®æ”¹å¯¼å…¥
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter

from src.config import settings


class RAGRetrieverFactory:
    """
    RAG æ£€ç´¢å™¨å·¥å‚ç±» (å•ä¾‹æ¨¡å¼)
    """

    def __init__(
        self,
        knowledge_base_path: Path = settings.KNOWLEDGE_BASE_PATH,
        embedding_model_name: str = settings.EMBEDDING_MODEL_NAME,
        chunk_size: int = settings.CHUNK_SIZE,
        chunk_overlap: int = settings.CHUNK_OVERLAP,
        search_k: int = settings.SEARCH_K,
    ):
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– RAG å‘é‡çŸ¥è¯†åº“ (ä»…ä¸€æ¬¡)...")
        self.config = {
            "path": knowledge_base_path,
            "embedding": embedding_model_name,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "search_k": search_k,
        }

        self.embeddings = HuggingFaceEmbeddings(model_name=self.config["embedding"])
        self.text_splitter = self._create_text_splitter()
        self.retriever = self._create_retriever()
        print("âœ… RAG å‘é‡çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆã€‚")

    def _create_text_splitter(self) -> TextSplitter:
        return RecursiveCharacterTextSplitter(
            chunk_size=self.config["chunk_size"],
            chunk_overlap=self.config["chunk_overlap"],
        )

    def _load_and_split_documents(self) -> List[Document]:
        # å¢åŠ æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
        file_path = Path(self.config["path"])
        if not file_path.exists():
            print(f"âš ï¸ è­¦å‘Š: çŸ¥è¯†åº“æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            return []

        loader = TextLoader(str(file_path), encoding="utf-8")
        documents = loader.load()
        return self.text_splitter.split_documents(documents)

    def _create_retriever(self) -> BaseRetriever:
        docs = self._load_and_split_documents()

        if not docs:
            print("âš ï¸ è­¦å‘Š: çŸ¥è¯†åº“ä¸ºç©ºï¼ŒåŠ è½½é»˜è®¤å ä½ç¬¦ã€‚")
            docs = [
                Document(
                    page_content="æš‚æ— ç›¸å…³å£å²¸æ³•è§„çŸ¥è¯†ã€‚",
                    metadata={"source": "empty_fallback"},
                )
            ]

        vectorstore = FAISS.from_documents(docs, self.embeddings)
        return vectorstore.as_retriever(search_kwargs={"k": self.config["search_k"]})

    def retrieve(self, query: str) -> str:
        """æ ¸å¿ƒæ£€ç´¢é€»è¾‘ï¼Œä¾› Tool è°ƒç”¨"""
        try:
            docs = self.retriever.invoke(query)
            if not docs:
                return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            return "\n\n".join(doc.page_content for doc in docs)
        except Exception as e:
            return f"æ£€ç´¢çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}"


# --- æ¨¡å—çº§å•ä¾‹ ---
rag_retriever_factory = RAGRetrieverFactory()


# --- âœ… å…³é”®ä¿®å¤ï¼šä½¿ç”¨ @tool è£…é¥°å™¨å®šä¹‰å·¥å…· ---
# è¿™æ ·èƒ½ç”Ÿæˆæ ‡å‡†çš„ JSON Schemaï¼Œé¿å… ChatTongyi/Qwen è§£æé”™è¯¯
@tool
def search_port_regulations(query: str) -> str:
    """
    æŸ¥è¯¢å®æ³¢å£å²¸çš„æµ·å…³æŸ¥éªŒæµç¨‹ã€H98æŒ‡ä»¤å«ä¹‰ã€äººå·¥æŸ¥éªŒæ—¶æ•ˆåŠåº”å¯¹ç­–ç•¥ç­‰æ³•è§„çŸ¥è¯†ã€‚
    å½“é‡åˆ°ä¸æ¸…æ¥šçš„æŸ¥éªŒçŠ¶æ€ï¼ˆå¦‚H98ï¼‰æˆ–éœ€è¦åº”å¯¹å»ºè®®æ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·ã€‚
    """
    return rag_retriever_factory.retrieve(query)


def get_rag_tool() -> BaseTool:
    """è·å–å…¨å±€RAGå·¥å…·å®ä¾‹ã€‚"""
    return search_port_regulations
